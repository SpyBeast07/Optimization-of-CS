{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+--------+-----------------------------------+---------------------+\n",
      "|   k |   x1 |             x2 |   f(x) | Gradient                          | Descent Direction   |\n",
      "+=====+======+================+========+===================================+=====================+\n",
      "|   0 |    1 |   2            |     12 | [ 0 11]                           | [  0 -11]           |\n",
      "+-----+------+----------------+--------+-----------------------------------+---------------------+\n",
      "|   1 |    1 |  -5.20652e+307 |   -inf | [5.20651939e+307            -inf] | [nan nan]           |\n",
      "+-----+------+----------------+--------+-----------------------------------+---------------------+\n",
      "|   2 |  nan | nan            |    nan | [nan nan]                         | [nan nan]           |\n",
      "+-----+------+----------------+--------+-----------------------------------+---------------------+\n",
      "Value of x: [nan nan]\n",
      "Minimum f(x): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260293/3609817663.py:7: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  return x[0]*2 - x[0]*x[1] + 3*x[1]*2\n",
      "/tmp/ipykernel_260293/3609817663.py:10: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  grad = np.array([2*x[0] - x[1], -x[0] + 6*x[1]])\n",
      "/tmp/ipykernel_260293/3609817663.py:29: RuntimeWarning: invalid value encountered in multiply\n",
      "  d = -grad + beta * d\n",
      "/tmp/ipykernel_260293/3609817663.py:29: RuntimeWarning: invalid value encountered in add\n",
      "  d = -grad + beta * d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "from tabulate import tabulate\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def func(x):\n",
    "    return x[0]*2 - x[0]*x[1] + 3*x[1]*2\n",
    "\n",
    "def gradient(x):\n",
    "    grad = np.array([2*x[0] - x[1], -x[0] + 6*x[1]])\n",
    "    return grad\n",
    "\n",
    "def conjugate_descent(x):\n",
    "    table = []\n",
    "    k=0\n",
    "    grad = gradient(x)\n",
    "    d = -grad\n",
    "    alpha = 0.01\n",
    "    table.append([k, x[0], x[1], func(x), grad, d])\n",
    "    while np.linalg.norm(grad) > 1e-6 and k < 50:\n",
    "        k += 1\n",
    "        x_prev = x.copy()\n",
    "        grad_prev = grad.copy()\n",
    "        def phi(alpha): return func(x_prev + alpha * d)\n",
    "        alpha = minimize_scalar(phi).x\n",
    "        x = x_prev + alpha * d\n",
    "        grad = gradient(x)\n",
    "        beta = max(0, np.dot(grad, grad - grad_prev) / np.dot(grad_prev, grad_prev))\n",
    "        d = -grad + beta * d\n",
    "        table.append([k, x[0], x[1], func(x), grad, d])\n",
    "    return table, x, func(x)\n",
    "\n",
    "x1 = 1\n",
    "x2 = 2\n",
    "x = np.array([x1,x2])\n",
    "table, x, ans = conjugate_descent(x)\n",
    "print(tabulate(table, headers=[\"k\", \"x1\", \"x2\", \"f(x)\", \"Gradient\", \"Descent Direction\"], tablefmt=\"grid\"))\n",
    "print(\"Value of x:\", x)\n",
    "print(\"Minimum f(x):\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of x after 3 iterations: [-1.   1.5]\n",
      "Optimal value of the function after 3 iterations: -1.25\n",
      "Hessian matrix:\n",
      "[[4 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def f(x): \n",
    "    x1, x2 = x[0], x[1] \n",
    "    return x1 - x2 + 2*x1**2 + 2*x1*x2 + x2**2\n",
    "\n",
    "def grad_f(x): \n",
    "    x1, x2 = x[0], x[1] \n",
    "    return np.array([1 + 4*x1 + 2*x2, -1 + 2*x1 + 2*x2]) \n",
    "\n",
    "def hessian_f(x): \n",
    "    return np.array([[4, 2], [2, 2]]) \n",
    "\n",
    "def conjugate_gradient_method(f, grad_f, hessian_f, x0, tol=1e-6, \n",
    "max_iter=100): \n",
    "    x = np.array(x0) \n",
    "    grad = grad_f(x) \n",
    "    d = -grad \n",
    "    delta_new = np.dot(grad, grad) \n",
    "    iter_count = 0 \n",
    "    x_values = [x]  \n",
    "    f_values = [f(x)]  \n",
    "    while iter_count < max_iter: \n",
    "        alpha = delta_new / np.dot(d, np.dot(hessian_f(x), d)) \n",
    "        x = x + alpha * d \n",
    "        grad = grad_f(x) \n",
    "        delta_old = delta_new \n",
    "        delta_new = np.dot(grad, grad) \n",
    "        beta = delta_new / delta_old \n",
    "        if np.linalg.norm(grad) < tol: \n",
    "            break \n",
    "        d = -grad + beta * d \n",
    "        iter_count += 1 \n",
    "        x_values.append(x) \n",
    "        f_values.append(f(x)) \n",
    "    return x, f(x), np.array(x_values), np.array(f_values) \n",
    "\n",
    "x0 = [0, 0] \n",
    "optimal_x, optimal_f, x_values, f_values = conjugate_gradient_method(f, \n",
    "grad_f, hessian_f, x0, max_iter=3) \n",
    "print(\"Optimal value of x after 3 iterations:\", optimal_x) \n",
    "print(\"Optimal value of the function after 3 iterations:\", optimal_f) \n",
    "print(\"Hessian matrix:\") \n",
    "27 \n",
    "\n",
    "print(hessian_f(x0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
