{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x + 2$"
      ],
      "text/plain": [
       "2*x + 2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "x = 'x**2 + 2*x'\n",
    "y = Derivative(x, symbols('x'))\n",
    "y.doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached\n",
      "Minimum found at: 1.0 with function value: 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate=0.01, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Performs steepest descent optimization.\n",
    "\n",
    "    Args:\n",
    "        f: The function to minimize.\n",
    "        grad_f: The gradient of the function f.\n",
    "        x0: The initial guess for the minimum.\n",
    "        learning_rate: The learning rate for updating the guess (default is 0.01).\n",
    "        tol: Tolerance for termination (default is 1e-6).\n",
    "        max_iter: Maximum number of iterations (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "        The minimum point found (as a numpy array) and its function value.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for k in range(max_iter):\n",
    "        # 1. Calculate gradient at current point\n",
    "        grad = grad_f(x)\n",
    "\n",
    "        # 2. Check termination condition\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            return x, f(x)\n",
    "\n",
    "        # 3. Search direction (negative gradient)\n",
    "        s = -grad\n",
    "\n",
    "        # 4. Line search for suitable step size (alpha)\n",
    "        # (Replace this with your implementation for Wolfe conditions)\n",
    "        alpha = line_search(f, x, s)\n",
    "\n",
    "        # 5. Update current guess\n",
    "        x = x + alpha * s\n",
    "\n",
    "    print(\"Maximum iterations reached\")\n",
    "    return x, f(x)\n",
    "\n",
    "# Example implementation of line search (replace with your own)\n",
    "def line_search(f, x, s):\n",
    "    # This is a simple example, replace with backtracking line search or other methods\n",
    "    alpha = 1.0\n",
    "    while f(x + alpha * s) > f(x):\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "\n",
    "# Example usage (replace with your own function and gradient)\n",
    "def f(x):\n",
    "    return x**2 + 2*x\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*x + 2\n",
    "\n",
    "x_min, f_min = gradient_descent(f, grad_f, 1.0)\n",
    "print(\"Minimum found at:\", x_min, \"with function value:\", f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached\n",
      "Minimum found at: (-0.9996372314316864, 1.4994130281264182) with function value: -1.249999818131844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent_2d(f, grad_f, x0, y0, learning_rate=0.01, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Performs gradient descent optimization for a 2-variable function.\n",
    "\n",
    "    Args:\n",
    "        f: The function to minimize.\n",
    "        grad_f: The gradient of the function f.\n",
    "        x0: The initial guess for the x variable.\n",
    "        y0: The initial guess for the y variable.\n",
    "        learning_rate: The learning rate for updating the guesses (default is 0.01).\n",
    "        tol: Tolerance for termination (default is 1e-6).\n",
    "        max_iter: Maximum number of iterations (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "        The minimum point found (as a tuple (x, y)) and its function value.\n",
    "    \"\"\"\n",
    "    x, y = x0, y0\n",
    "    for k in range(max_iter):\n",
    "        # 1. Calculate gradient at current point\n",
    "        grad_x, grad_y = grad_f(x, y)\n",
    "\n",
    "        # 2. Check termination condition\n",
    "        if np.linalg.norm([grad_x, grad_y]) < tol:\n",
    "            return (x, y), f(x, y)\n",
    "\n",
    "        # 3. Search direction (negative gradient)\n",
    "        s_x, s_y = -grad_x, -grad_y\n",
    "\n",
    "        # 4. Update current guesses\n",
    "        x = x + learning_rate * s_x\n",
    "        y = y + learning_rate * s_y\n",
    "\n",
    "    print(\"Maximum iterations reached\")\n",
    "    return (x, y), f(x, y)\n",
    "\n",
    "# Example 2-variable function\n",
    "def f_2d(x, y):\n",
    "    return x - y + 2*x**2 + 2*x*y + y**2\n",
    "\n",
    "# Gradient of the example function\n",
    "def grad_f_2d(x, y):\n",
    "    return 1 + 4*x + 2*y, -1 + 2*x + 2*y\n",
    "\n",
    "# Initial guesses\n",
    "x0, y0 = 1.0, 1.0\n",
    "\n",
    "# Perform gradient descent\n",
    "(min_point, min_value) = gradient_descent_2d(f_2d, grad_f_2d, x0, y0)\n",
    "\n",
    "print(\"Minimum found at:\", min_point, \"with function value:\", min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Minimize cable installation cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached\n",
      "Minimum found at: 0.7499999719312124 with function value: 48000.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def steepest_descent(f, grad_f, x0, learning_rate=0.01, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Performs steepest descent optimization.\n",
    "\n",
    "    Args:\n",
    "        f: The function to minimize.\n",
    "        grad_f: The gradient of the function f.\n",
    "        x0: The initial guess for the minimum.\n",
    "        learning_rate: The learning rate for updating the guess (default is 0.01).\n",
    "        tol: Tolerance for termination (default is 1e-6).\n",
    "        max_iter: Maximum number of iterations (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "        The minimum point found (as a numpy array) and its function value.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for k in range(max_iter):\n",
    "        # 1. Calculate gradient at current point\n",
    "        grad = grad_f(x)\n",
    "\n",
    "        # 2. Check termination condition\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            return x, f(x)\n",
    "\n",
    "        # 3. Search direction (negative gradient)\n",
    "        s = -grad\n",
    "\n",
    "        # 4. Line search for suitable step size (alpha)\n",
    "        # (Replace this with your implementation for Wolfe conditions)\n",
    "        alpha = line_search(f, x, s)\n",
    "\n",
    "        # 5. Update current guess\n",
    "        x = x + alpha * s\n",
    "\n",
    "    print(\"Maximum iterations reached\")\n",
    "    return x, f(x)\n",
    "\n",
    "# Example implementation of line search (replace with your own)\n",
    "def line_search(f, x, s):\n",
    "    # This is a simple example, replace with backtracking line search or other methods\n",
    "    alpha = 1.0\n",
    "    while f(x + alpha * s) > f(x):\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "\n",
    "def f(x):\n",
    "    return 15000 * math.sqrt(x**2 + 1) + 9000 * (4-x)\n",
    "\n",
    "def grad_f(x):\n",
    "    return (15000 * x / math.sqrt(x**2 + 1)) - 9000\n",
    "\n",
    "x_min, f_min = steepest_descent(f, grad_f, 0)\n",
    "print(\"Minimum found at:\", x_min, \"with function value:\", f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680.0\n"
     ]
    }
   ],
   "source": [
    "x = 3/4\n",
    "func = '(15000 / math.sqrt((x**2 + 1)**3))'\n",
    "result = eval(func)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
